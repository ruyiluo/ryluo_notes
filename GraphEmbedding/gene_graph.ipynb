{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_data_path = \"./data/action_head.csv\"\n",
    "product_data_path = \"./data/jdata_product.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_action_data():\n",
    "    action_data_df = pd.read_csv(action_data_path, \n",
    "                                 usecols=['user_id', 'sku_id', 'action_time', 'type'],\n",
    "                                 parse_dates=['action_time'])\n",
    "    # 将缺失的side info sku删除\n",
    "    jdata_product_df = pd.read_csv(product_data_path, \n",
    "                                   usecols=['sku_id', 'brand', 'shop_id', 'cate'])\n",
    "    \n",
    "    jdata_product_df = jdata_product_df.dropna()\n",
    "    \n",
    "    # 只保留在行为序列中出现过的sku的 side info\n",
    "    side_info_data_df = pd.merge(action_data_df, jdata_product_df, on='sku_id', how='left')\n",
    "    \n",
    "    # 对sku的side info类别编码\n",
    "    for side_info in ['brand', 'shop_id', 'cate']:\n",
    "        lbe = LabelEncoder()\n",
    "        side_info_data_df[side_info] = lbe.fit_transform(side_info_data_df[side_info])\n",
    "    \n",
    "    # 统计sku_id的频率\n",
    "    sku_count_dict = side_info_data_df.groupby('sku_id')['user_id'].count().to_dict()\n",
    "    \n",
    "    # 按照频率降序排序（为了后续使用sampled_softmax_loss）\n",
    "    sku_id_sort_by_count_list = sorted(sku_count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    all_sku_ids = [x[0] for x in sku_id_sort_by_count_list]\n",
    "    \n",
    "    # 对sku做类别编码\n",
    "    sku_map_dict = {}\n",
    "    index2id_dict = {}\n",
    "    for i, sku_id in enumerate(all_sku_ids):\n",
    "        sku_map_dict[sku_id] = i\n",
    "        index2id_dict[i] = sku_id\n",
    "        \n",
    "    side_info_data_df['sku_id'] = side_info_data_df['sku_id'].map(sku_map_dict)\n",
    "    \n",
    "    # 填充缺失值\n",
    "    side_info_data_df = side_info_data_df.fillna(0)\n",
    "    \n",
    "    return side_info_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过用户的行为序列，构造多个session,最终是通过session来构建图\n",
    "def get_one_user_session(df, time_cut=30, cut_type=2):\n",
    "    \"\"\"\n",
    "    time_cut: 指的是多长时间的序列做截断\n",
    "    cut_type: 指的是如果最后一次是下单也做截断\n",
    "    \"\"\"\n",
    "    # 先把当前用户的sku_list, time_list, 以及type_list取出来\n",
    "    sku_list = df['sku_id']\n",
    "    action_time_list = df['action_time']\n",
    "    action_type_list = df['type']\n",
    "    user_session_list = []\n",
    "    tmp_session_list = []\n",
    "    # 遍历用户的整个行为序列\n",
    "    for i, sku in enumerate(sku_list):\n",
    "        # 如果当前sku是下单、浏览时间间隔为30分钟(指的是当前sku距离下一次浏览的sku的间隔)、\n",
    "        # 或者是最后一个sku则对当前的session做截断\n",
    "        if action_time_list[i] == cut_type or i == len(sku_list) - 1 or \\\n",
    "            (i < len(sku_list) -1 and (action_time_list[i+1] - action_time_list[i]).\\\n",
    "             seconds/60 > 30):\n",
    "            tmp_session_list.append(sku)\n",
    "            user_session_list.append(tmp_session_list)\n",
    "            tmp_session_list = []\n",
    "        else:\n",
    "            tmp_session_list.append(sku)\n",
    "    return user_session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_all_user_session(action_data_df):\n",
    "    action_types = [1, 2, 3, 4, 5]\n",
    "    action_data_df = action_data_df[action_data_df['type'].isin(action_types)]\n",
    "\n",
    "    # 按照时间，从小到大进行排序\n",
    "    action_data_df = action_data_df.sort_values(['user_id', 'action_time'], ascending=True)\n",
    "\n",
    "    # 将用户的行为序列聚合在一起，并根据规则将用户的行为序列切分成多个session\n",
    "    user_action_list = action_data_df.groupby('user_id').agg(list).apply(get_one_user_session, axis=1)\n",
    "\n",
    "    # 将session中长度大于1的都保存起来构图\n",
    "    final_user_session_list = []\n",
    "\n",
    "    for user_sessions in user_action_list:\n",
    "        for session in user_sessions:\n",
    "            if len(session) > 1:\n",
    "                final_user_session_list.append(session)\n",
    "\n",
    "    return final_user_session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_graph(all_user_session_list):\n",
    "    edges_dict = {} # 字典的key表示的是边，value表示的是这条边出现的次数，也可以认为是权重\n",
    "    for session in all_user_session_list:\n",
    "        for i in range(len(session) - 1):\n",
    "            if (session[i], session[i + 1]) not in edges_dict:\n",
    "                edges_dict[(session[i], session[i + 1])] = 1\n",
    "            else:\n",
    "                edges_dict[(session[i], session[i + 1])] += 1\n",
    "\n",
    "    # 为了方便处理，一般会把图结构存下来\n",
    "    src_nodes_list = [x[0] for x in list(edges_dict.keys())]\n",
    "    dst_nodes_list = [x[1] for x in list(edges_dict.keys())]\n",
    "    edge_weight_list = list(edges_dict.values())\n",
    "\n",
    "    graph_df = pd.DataFrame({'src_node': src_nodes_list, 'dst_node': dst_nodes_list,\n",
    "                            'weight': edge_weight_list})\n",
    "    # 保存图结构的时候，用空格隔开，方便后面读取图\n",
    "    graph_df.to_csv('./graph.csv', sep=' ', header=False, index=False)\n",
    "\n",
    "    # 从边文件中构造图结构\n",
    "    G = nx.read_edgelist('./graph.csv', create_using=nx.DiGraph(), nodetype=None, data=[('weight', int)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 读取用户行为数据\n",
    "    side_info_data_df = read_action_data()\n",
    "    # 生成所有用户的session\n",
    "    all_user_session_list = gene_all_user_session(side_info_data_df[\\\n",
    "                            ['user_id','sku_id', 'action_time', 'type']])\n",
    "    # 根据用户session构图\n",
    "    gene_graph(all_user_session_list)\n",
    "    \n",
    "    # 保存side info特征\n",
    "    side_info_data_df.to_csv('./sideinfo.csv', sep=',', header=True, index=False)\n",
    "    \n",
    "    # sku_side_info\n",
    "    sku_side_info = side_info_data_df[['sku_id', 'brand', 'shop_id', \\\n",
    "                        'cate']].drop_duplicates(subset=['sku_id'])\n",
    "    sku_side_info.to_csv('./sku_sideinfo.csv', sep=',', header=True, index=False)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
