{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import networkx as nx \n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from alias import create_alias_tables, alias_sample\n",
    "from feature_column import SparseFeat\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.initializers import RandomNormal, Zeros, glorot_normal\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Flatten, Concatenate, Dense, Reshape\n",
    "\n",
    "from features import FeatureEncoder\n",
    "from utils import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带权的deepwalk随机游走\n",
    "def deepwalk_weight(G, walk_length=10):\n",
    "    \"\"\"\n",
    "    1. 遍历图中所有节点，以每个节点为起始节点采样\n",
    "    2. 用带权邻居采样代替随机采样\n",
    "    \"\"\"\n",
    "    nodes_list = G.nodes\n",
    "    \n",
    "    # 处理每个节点邻居节点的采样概率，并转化成alias表的形式\n",
    "    node_alias_dict = {}\n",
    "    for node in tqdm(nodes_list):\n",
    "        nbrs_prob_list = [G[node][nbr].get('weight', 1.0) \\\n",
    "                          for nbr in list(G.neighbors(node))]\n",
    "        normalized_prob_list = [float(prob) / sum(nbrs_prob_list)\\\n",
    "                               for prob in nbrs_prob_list]\n",
    "        # 构建alias表\n",
    "        node_alias_dict[node] = create_alias_tables(normalized_prob_list)\n",
    "    \n",
    "    # 遍历每个节点，以每个节点为起始节点进行采样\n",
    "    sentenses_list = []\n",
    "    for node in tqdm(nodes_list):\n",
    "        tmp_sentence_list = [node]\n",
    "        \n",
    "        while len(tmp_sentence_list) < walk_length:\n",
    "            cur_node = tmp_sentence_list[-1]\n",
    "            nbrs_list = list(G.neighbors(cur_node))\n",
    "            \n",
    "            # 如果当前节点没有邻居节点，则停止采样\n",
    "            if len(nbrs_list) == 0:\n",
    "                break\n",
    "            \n",
    "            # 获取当前节点邻居节点的alias表\n",
    "            accept_prob = node_alias_dict[cur_node][0]\n",
    "            alias_table = node_alias_dict[cur_node][1]\n",
    "            sample_index = alias_sample(accept_prob, alias_table)\n",
    "            tmp_sentence_list.append(nbrs_list[sample_index])\n",
    "        \n",
    "        sentenses_list.append(tmp_sentence_list)\n",
    "    \n",
    "    return sentenses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pairs(sentences, window_size=5):\n",
    "    \"\"\"\n",
    "    1. 遍历所有的序列，根据设定的窗口，获取共现的pair\n",
    "    \"\"\"\n",
    "    \n",
    "    all_pairs_list = []\n",
    "    \n",
    "    # 遍历所有序列\n",
    "    for sentence in tqdm(sentences):\n",
    "        # 遍历序列的每个位置\n",
    "        for i in range(len(sentence)):\n",
    "            # 在当前元素的前、后窗口中构造pair\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                # 如果窗口中的元素等于当前元素，或者当前窗口不合法\n",
    "                if i == j or j < 0 or j >= len(sentence):\n",
    "                    continue\n",
    "                # 构造pair\n",
    "                all_pairs_list.append((sentence[i], sentence[j]))\n",
    "                \n",
    "    # 用np规整一下所有的格式，防止索引中存在float类型的数据\n",
    "    return np.array(all_pairs_list, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(all_paris):\n",
    "    # 构造训练样本，就是把side info考虑进去\n",
    "    # 数据格式：  side_info_sku_id, brand, cate, shop_id, sku_id  (no side_info)\n",
    "    sku_side_info_df = pd.read_csv('./sku_sideinfo.csv', dtype=np.int)\n",
    "\n",
    "    all_pairs_df = pd.DataFrame(all_paris, columns=['side_info_sku_id', 'sku_id_pair'], dtype=np.int)\n",
    "\n",
    "    train_df = pd.merge(all_pairs_df, sku_side_info_df, \n",
    "                        left_on='side_info_sku_id', right_on='sku_id')\n",
    "\n",
    "    # 对sku的side info类别编码\n",
    "    for side_info in ['brand', 'shop_id', 'cate']:\n",
    "        lbe = LabelEncoder()\n",
    "        train_df[side_info] = lbe.fit_transform(train_df[side_info])\n",
    "    \n",
    "    feature_max_index_dict = {}\n",
    "    feature_max_index_dict['side_info_sku_id'] = train_df['side_info_sku_id'].max() + 1\n",
    "    feature_max_index_dict['brand'] = train_df['brand'].max() + 1\n",
    "    feature_max_index_dict['shop_id'] = train_df['shop_id'].max() + 1\n",
    "    feature_max_index_dict['cate'] = train_df['cate'].max() + 1\n",
    "    feature_max_index_dict['sku_id_pair'] = train_df['sku_id_pair'].max() + 1\n",
    "\n",
    "    feature_names = ['side_info_sku_id', 'brand', 'shop_id', 'cate', 'sku_id_pair']\n",
    "    train_input_dict = {}\n",
    "    for name in feature_names:\n",
    "        train_input_dict[name] = np.array(train_df[name].values) \n",
    "\n",
    "    train_label = np.array([1] * len(train_input_dict))\n",
    "\n",
    "    return feature_max_index_dict, train_input_dict, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 31364/31364 [00:00<00:00, 91531.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 31364/31364 [00:00<00:00, 58780.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 27095/27095 [00:00<00:00, 50486.89it/s]\n",
      "/tmp/ipykernel_21729/2579947807.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  sku_side_info_df = pd.read_csv('./sku_sideinfo.csv', dtype=np.int)\n",
      "/tmp/ipykernel_21729/2579947807.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  all_pairs_df = pd.DataFrame(all_paris, columns=['side_info_sku_id', 'sku_id_pair'], dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# 读取图数据\n",
    "G = nx.read_edgelist('./graph.csv', create_using=nx.DiGraph(), \\\n",
    "                     nodetype=None, data=[('weight', int)])\n",
    "# 随机游走获取新的序列\n",
    "sentences = deepwalk_weight(G)\n",
    "# 过滤长度小于2的序列\n",
    "sentences = [s for s in sentences if len(s) >= 2]\n",
    "\n",
    "# 根据序列构造pairs\n",
    "all_paris = get_all_pairs(sentences, window_size=5)\n",
    "\n",
    "feature_max_index_dict, train_input_dict, train_label = get_train_data(all_paris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGES(sparse_feature_columns, sideinfo_features, \n",
    "         item_pair_feature_name,num_sampled=5):\n",
    "    \"\"\"EGES模型\n",
    "    \"\"\"\n",
    "    # 获取所有的feature_columns\n",
    "    feature_columns = sparse_feature_columns\n",
    "    # 根据feature_colimns 构建input层和Embedding层，并将input层和Embedding层串起来\n",
    "    feature_encode = FeatureEncoder(feature_columns)\n",
    "    # 获取模型的输入，其实就是所有的Input层\n",
    "    feature_input_layers_list = list(feature_encode.feature_input_layer_dict.values())\n",
    "        \n",
    "    # 将所有的sparse特征拿出来\n",
    "    group_embedding_dict = feature_encode.sparse_feature_dict\n",
    "        \n",
    "    # 将所有的side info特征拿出来\n",
    "    side_info_embedding_list = [v for k, v in group_embedding_dict['default_group'].items() \n",
    "        if k in sideinfo_features]\n",
    "        \n",
    "    # 把sku embedding层拿出来, 就是后面那个大矩阵层\n",
    "    sku_embedding_layer = feature_encode.embedding_layers_dict['sku_id_pair']\n",
    "    \n",
    "    # 方便后面获取注意力权重\n",
    "    side_info_sku_input_layer = feature_input_layers_list[0]\n",
    "    \n",
    "    # 注意只有一个\n",
    "    item_pair_input = feature_encode.feature_input_layer_dict['sku_id_pair']\n",
    "        \n",
    "    item_vocabulary_size = sparse_feature_columns[0].vocabulary_size \n",
    "    embedding_dim = sparse_feature_columns[0].embedding_dim\n",
    "    \n",
    "    # (B, embedding_dim)\n",
    "    side_info_pooling_output = EGESPooling(item_vocabulary_size, len(sideinfo_features))\\\n",
    "                    ([side_info_embedding_list, side_info_sku_input_layer])\n",
    "        \n",
    "    # 把所有的item 的embedding都拿出来\n",
    "    item_index = EmbeddingIndex(list(range(item_vocabulary_size)))(feature_input_layers_list[0])\n",
    "    item_embedding_weight = NoMask()(sku_embedding_layer(item_index))\n",
    "\n",
    "    softmax_input = (item_embedding_weight, side_info_pooling_output, item_pair_input)\n",
    "    \n",
    "    # 采样的softmax\n",
    "    output = SampledSoftmaxLayer(num_sampled)(softmax_input)\n",
    "    \n",
    "    model = Model(feature_input_layers_list, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造模型\n",
    "\n",
    "embedding_dim = 16\n",
    "side_info_feature_name = ['side_info_sku_id', 'brand', 'shop_id', 'cate']\n",
    "item_pair_feature_name = ['sku_id_pair']\n",
    "\n",
    "sparse_feature_columns = [SparseFeat(name, vocabulary_size=feature_max_index_dict[name], \\\n",
    "                        embedding_dim=embedding_dim, embedding_name=name) for name in \\\n",
    "                        side_info_feature_name + item_pair_feature_name]\n",
    "\n",
    "model = EGES(sparse_feature_columns, side_info_feature_name, item_pair_feature_name)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \\\n",
    "              loss=sampledsoftmaxloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryluo/anaconda3/envs/tf2.2/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-05-02 18:56:17.461291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007/1007 [==============================] - 12s 12ms/step - loss: 14.7934 - val_loss: 2.1919\n",
      "Epoch 2/2\n",
      "1007/1007 [==============================] - 11s 11ms/step - loss: 1.3425 - val_loss: 1.0312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcadd374580>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input_dict, train_label, batch_size=1024, epochs=2, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
